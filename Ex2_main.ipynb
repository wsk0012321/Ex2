{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7825f6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register the pre-trained embedding model to spacy, please repalce the directories with your own\n",
    "# also you need to input the abbreviation of langauge: en for english, sv for swedish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a749dc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy init vectors sv D:/embeddings/sv_talbanken.txt D:/models/sv --name model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e01b2a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy,easygui,os\n",
    "root = os.getcwd() # define default root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912b50e2",
   "metadata": {},
   "source": [
    "## import test set from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86729694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu\n",
    "\n",
    "# select corpus file\n",
    "file = easygui.fileopenbox(default = root, title = \"Select test data\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1806115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read annotated data from corpus\n",
    "annotations = open(file, encoding = \"utf-8-sig\").read()\n",
    "sentences = conllu.parse(annotations)\n",
    "# extract sentences for parsing\n",
    "sent_list = [sent.metadata[\"text\"]  for sent in sentences]\n",
    "\n",
    "# extract annotated dependency data for comparing\n",
    "dep_annotated = [] # overall dependency with each sentence as a sublist                     \n",
    "for sent in sentences:\n",
    "    dependency = [] # dependency with a single sentence\n",
    "    for token in sent:\n",
    "        if token[\"head\"] == 0:\n",
    "            # convert the token id to its index in list if the head is root\n",
    "            head = token[\"form\"]\n",
    "        elif type(token[\"head\"]) == int: # when head is labelled, we can get the \"head\" as int\n",
    "            # get index of the head\n",
    "            head_index = token[\"head\"] - 1 # in the corpus the token id strats from 1, while index of list strats from 0\n",
    "            # retrieve the head \n",
    "            head = sent[head_index][\"form\"]\n",
    "                    \n",
    "        else: # when no head is labelled, the type of this parameter should be NoneType. we ignore such situation\n",
    "            pass\n",
    "        # insert token, dependency and head\n",
    "        dependency.append((token[\"form\"], token[\"deprel\"], head))\n",
    "        \n",
    "    # append the parsed dependency to the overall list\n",
    "    dep_annotated.append(dependency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fd423b",
   "metadata": {},
   "source": [
    "## import the training set from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "726a73b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select data for training\n",
    "file_train = easygui.fileopenbox(default = root, title = \"Select train data\")\n",
    "sents_train = conllu.parse(open(file_train, encoding = \"utf-8-sig\").read())\n",
    "\n",
    "# arrange the data in format [(\"text\", {\"heads\": heads, \"deps\": deps }), ...]\n",
    "train_data = []\n",
    "for sent in sents_train:\n",
    "    # extract head index\n",
    "    heads = []\n",
    "    deps = []\n",
    "    for token in sent:\n",
    "            # when the token is the root, the value of head is 0. we manually set it to its only index\n",
    "            if token[\"head\"] == 0:\n",
    "                # convert the token id to its index in list\n",
    "                heads.append(token[\"id\"] - 1)\n",
    "                deps.append(token[\"deprel\"])\n",
    "            # otherwise, when the id is int other than 0, the token has another one as its head \n",
    "            elif type(token[\"id\"]) == int:\n",
    "                heads.append(token[\"head\"] - 1)\n",
    "                deps.append(token[\"deprel\"])\n",
    "            else:\n",
    "            # in some case the head is not assigned in corpus and the type of id would be Nonetype. So we pop out such elements\n",
    "                pass\n",
    "    # combine deps and heads\n",
    "    dep_dict = {\"heads\": heads,\"deps\": deps}\n",
    "    # insert raw text\n",
    "    train_data.append((sent.metadata[\"text\"], dep_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b10cc7",
   "metadata": {},
   "source": [
    "## prepare the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dde7524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168  sentences of  4303 are removed.\n"
     ]
    }
   ],
   "source": [
    "# if if the result of the tokenization by spacy is different from the annotated data\n",
    "# the training will be blocked\n",
    "# so we need to remove such data\n",
    "save_model = easygui.diropenbox(default = root, title = \"Directory of saved model\")\n",
    "# load trained model with embedding\n",
    "nlp = spacy.load(save_model)\n",
    "\n",
    "len_original = len(train_data) # length of the original train data\n",
    "count_removed = 0\n",
    "\n",
    "from spacy.training import Example\n",
    "\n",
    "examples = []\n",
    "for item in train_data:\n",
    "    doc = nlp(item[0])\n",
    "    # get the token list\n",
    "    token_spacy = [token.text for token in doc]\n",
    "\n",
    "    # if the result of the tokenization by spacy is different from the annotated data\n",
    "    if len(item[1][\"deps\"]) != len(token_spacy) or len(item[1][\"heads\"]) != len(token_spacy):\n",
    "        # the item will be popped out\n",
    "        train_data.remove(item)\n",
    "        count_removed += 1\n",
    "    else:\n",
    "        # create examples\n",
    "        examples.append(Example.from_dict(doc, item[1]))\n",
    "\n",
    "# the loss of train data\n",
    "print(count_removed, \" sentences of \", len_original, \"are removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70363f5",
   "metadata": {},
   "source": [
    "## train the parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1be35748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Continue.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add parser\n",
    "nlp.add_pipe(\"parser\")\n",
    "\n",
    "# gather the labels from annotated corpus\n",
    "labels = []\n",
    "for sent in sents_train:\n",
    "    for token in sent:\n",
    "        if token[\"deprel\"] not in labels:\n",
    "            labels.append(token[\"deprel\"])\n",
    "\n",
    "# add label to the default parser\n",
    "parser = nlp.get_pipe(\"parser\")\n",
    "for label in labels:\n",
    "    if label not in parser.labels:\n",
    "        parser.add_label(label)\n",
    "        \n",
    "# training\n",
    "\n",
    "import random\n",
    "\n",
    "# get number of iterations\n",
    "training_iterations = easygui.integerbox(msg=\"How many times of iteration:\", lowerbound = 1)\n",
    "\n",
    "# initialize the model\n",
    "optimizer = nlp.initialize(lambda: examples )\n",
    "for i in range(training_iterations):\n",
    "    random.shuffle(examples)\n",
    "    # update the model\n",
    "    nlp.update(examples, sgd = optimizer) \n",
    "\n",
    "nlp.to_disk(save_model)\n",
    "\n",
    "easygui.msgbox(msg = \"Finished training!\", title = None, ok_button = \"Continue.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8664cc4b",
   "metadata": {},
   "source": [
    "## parsing with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2593088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the trained model\n",
    "nlp2 = spacy.load(save_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9ff18e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract dependency data from the result\n",
    "dep_spacy = [] # overall dependency as result of spacy   \n",
    "for sent in sent_list:\n",
    "    dependency = [] # dependency withi each sentence\n",
    "    for token in nlp2(sent):\n",
    "        # insert token, dependency and head\n",
    "        dependency.append((token.text, token.dep_.lower(), token.head.text))\n",
    "    # append to the overall list\n",
    "    dep_spacy.append(dependency)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de069316",
   "metadata": {},
   "source": [
    "## comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04667a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_mismatch = []\n",
    "token_count = 0\n",
    "mismatch_count = 0\n",
    "\n",
    "for n in range(0, len(dep_spacy)):\n",
    "    # we assume that the sentences are tokenized in the same way both in spacy and in corpus, so mismatch of tokenization is ignored\n",
    "    if len(dep_annotated[n]) == len(dep_spacy[n]):\n",
    "            # renew number of token\n",
    "            token_count += len(dep_spacy[n])\n",
    "            for a in range(0,len(dep_spacy[n])): # iterate through all the tokens in a sentence \n",
    "                 if dep_spacy[n][a] != dep_annotated[n][a]: # check if the dependency relations are identical\n",
    "                        mismatch_count += 1\n",
    "                        token_mismatch.append((dep_annotated[n][a],dep_spacy[n][a]))\n",
    "# calculate accuracy                        \n",
    "precision =  1 - (mismatch_count / token_count)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d76650",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The precision of spacy dependency parser is %d.\" % precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6870ad2",
   "metadata": {},
   "source": [
    "## save as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15b1aa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# to data frame\n",
    "df = pd.DataFrame.from_records(token_mismatch, columns = [\"Corpus\", \"Spacy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e1be087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input directory\n",
    "directory = easygui.diropenbox(default = root, msg = \"Save the csv\")\n",
    "# name the file\n",
    "filename = easygui.enterbox(msg = \"Name your file\")\n",
    "#to csv\n",
    "df.to_csv((directory + \"/\" + filename + \".csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac673b2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
